{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feQUB2NIhVmd"
      },
      "source": [
        "# GenesisMind-Building-GPT1-from-Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-JliMBxhYej"
      },
      "source": [
        "GPT-1 \"Generative Pre-trained Transformer\" is the first version of the GPT series of models, revolutionized natural language processing with its autoregressive language modeling capabilities built on the Transformer architecture. This repository serves as a comprehensive guide to understanding and implementing the GPT model. I'm gonna walk through the essential components, techniques, and principles behind GPT.\n",
        "\n",
        "**Note:** This is a mini-GPT model for text generation. The original model is very large and deals with massive data, so of course here we will not be using the same dataset or even the same size of model. However, you can control the parameters or even modify it easily if necessary to match the original model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The environment"
      ],
      "metadata": {
        "id": "esc8V3d7gw07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are creating this demo within Jupyter Notebook, so first of all don't forget to go to the working folder:"
      ],
      "metadata": {
        "id": "co-ryLoYg3n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/projects/GenesisMind-Building-GPT1-from-Scratch'"
      ],
      "metadata": {
        "id": "HTEEvMKvhLKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download requirements"
      ],
      "metadata": {
        "id": "YTyC0-qcgaF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "1mxVEmUsgbGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iufwkhmGuYJG"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the validity of our work, we will work with a relatively small dataset of 50,000 samples. IMDB popular movie data collection."
      ],
      "metadata": {
        "id": "ba0oiLqKf2Sc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21189qBDugs7"
      },
      "outputs": [],
      "source": [
        "#!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the dataset"
      ],
      "metadata": {
        "id": "8lstM-qyhbUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All we need to do is create another folder in which we place a specific number of samples for validation. Of course you can change that. We will use 5000 samples for validation."
      ],
      "metadata": {
        "id": "0Xw-U9uJiP24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import multiprocessing\n",
        "\n",
        "# Define a function for moving files from one directory to another\n",
        "def move_files(src_files, dest_dir):\n",
        "    for file_to_move in src_files:\n",
        "        destination_path = os.path.join(dest_dir, os.path.basename(file_to_move))\n",
        "        try:\n",
        "            shutil.move(file_to_move, destination_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving file {file_to_move}: {str(e)}\")\n",
        "\n",
        "# Define your source and validation directories\n",
        "test_dir = \"aclImdb/test\"\n",
        "validation_dir = \"aclImdb/valid\"\n",
        "num_files_to_move = 2500\n",
        "\n",
        "# Create the validation directory if it doesn't exist\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "\n",
        "# Create separate 'pos' and 'neg' subdirectories within the validation directory\n",
        "validation_pos_dir = os.path.join(validation_dir, 'pos')\n",
        "validation_neg_dir = os.path.join(validation_dir, 'neg')\n",
        "os.makedirs(validation_pos_dir, exist_ok=True)\n",
        "os.makedirs(validation_neg_dir, exist_ok=True)\n",
        "\n",
        "# Define the subdirectories\n",
        "test_pos_dir = os.path.join(test_dir, 'pos')\n",
        "test_neg_dir = os.path.join(test_dir, 'neg')\n",
        "\n",
        "# List files in the test 'pos' and 'neg' directories\n",
        "test_pos_files = [os.path.join(test_pos_dir, filename) for filename in os.listdir(test_pos_dir)]\n",
        "test_neg_files = [os.path.join(test_neg_dir, filename) for filename in os.listdir(test_neg_dir)]\n",
        "\n",
        "# Randomly shuffle the lists of files\n",
        "random.seed(42)  # Set a random seed for reproducibility\n",
        "random.shuffle(test_pos_files)\n",
        "random.shuffle(test_neg_files)\n",
        "\n",
        "# Split the files into chunks for parallel processing\n",
        "chunk_size = num_files_to_move // multiprocessing.cpu_count()\n",
        "test_pos_chunks = [test_pos_files[i:i + chunk_size] for i in range(0, num_files_to_move, chunk_size)]\n",
        "test_neg_chunks = [test_neg_files[i:i + chunk_size] for i in range(0, num_files_to_move, chunk_size)]\n",
        "\n",
        "# Create a pool of worker processes\n",
        "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "\n",
        "# Move files in parallel\n",
        "pool.starmap(move_files, [(chunk, validation_pos_dir) for chunk in test_pos_chunks])\n",
        "pool.starmap(move_files, [(chunk, validation_neg_dir) for chunk in test_neg_chunks])\n",
        "\n",
        "# Close the pool of worker processes\n",
        "pool.close()\n",
        "pool.join()\n"
      ],
      "metadata": {
        "id": "VIDUhZK7_4ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_vS8BRGMkcv"
      },
      "source": [
        "## Byte-Pair Encoding (BPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4brDWQsTahd"
      },
      "source": [
        "The tokenizer used in GPT-2 is the same as the one used in GPT-1. Both GPT-1 and GPT-2 use the Byte-Pair Encoding (BPE) tokenizer, which breaks down words into subword units based on their frequency in the training data. This allows the tokenizer to handle out-of-vocabulary words and create a more compact vocabulary.\n",
        "\n",
        "The Hugging Face Transformers library provides a unified interface for various models, including GPT-1 and GPT-2. When you use the AutoTokenizer class from the library, it loads the appropriate tokenizer based on the model name you provide. Since GPT-1 and GPT-2 share the same tokenizer architecture, you can use the same tokenizer for both models.\n",
        "\n",
        "We can build the tokenizer from scratch or we can use a ready-made implementation of it. I don't like the idea of rebuilding it at all, it's a tedious and complex process, and it seems off topic. Common practice is to reuse and adapt the same implementation on a new dataset or corpus.\n",
        "\n",
        "It should be noted that tokenizer training continues until a certain number of iterations is reached, a certain number of vocabulary are reached, convergence is reached, or until the algorithm finds no new pairs to combine (in the latter case we get all the unique vocabulary in the corpus). That is, there are 4 different ways the training can be stopped. In this repo, we will use the most popular method, the second method (when training from scratch, I think the third method is the best, but it is more tiring than the other methods).\n",
        "\n",
        "We already know that the GPT2 model has a vocab size of 50,257 I think, so I can stop the algorithm when I get to 50,357. I think that will suffice and more since we are dealing with a relatively small and familiar data set. Actually, we never need to retrain the tokenizer for this dataset, but it's nice to get our hands dirty sometimes. However, you can still skip this step and use the tokenizer without training it.\n",
        "\n",
        "I will not explain the details of the algorithm's work, but [here](https://leimao.github.io/blog/Byte-Pair-Encoding/) there is a clear explanation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMEALSdrMll2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class BPETrainer():\n",
        "    def __init__(self, model=\"gpt2\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "    def train(self, data_dir, batch_size=1000, vocab_size=50357, save=True, save_fp='tokenizer/adapted-tokenizer'):\n",
        "        \"\"\"\n",
        "        Retrains GPT tokenizer on a new corpus.\n",
        "\n",
        "        Args:\n",
        "            data_dir (str): Corpus directory path.\n",
        "            batch_size (int): Batch size for reading files. Default is 1000.\n",
        "            vocab_size (int): Target vocabulary size for adapted tokenizer. Default is 50000.\n",
        "            save (bool): Whether to save the adapted tokenizer. Default is True.\n",
        "            save_fp (str): File path to save the adapted tokenizer. Default is 'tokenizer/adapted-tokenizer'.\n",
        "        \"\"\"\n",
        "        training_corpus = self.read_batch_of_files(data_dir, batch_size)\n",
        "        self.tokenizer = self.tokenizer.train_new_from_iterator(training_corpus, vocab_size)\n",
        "        if save:\n",
        "            self.save(save_fp)\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        return self.tokenizer.encode(sentence)\n",
        "\n",
        "    def read_batch_of_files(self, data_dir, batch_size, num_workers=4):\n",
        "        filenames = self.get_filenames(data_dir)\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "            for start_idx in range(0, len(filenames), batch_size):\n",
        "                batch_filenames = filenames[start_idx : start_idx + batch_size]\n",
        "                batch_contents = []\n",
        "                future_to_filename = {executor.submit(self.read_file, filename): filename for filename in batch_filenames}\n",
        "                for future in concurrent.futures.as_completed(future_to_filename):\n",
        "                    filename = future_to_filename[future]\n",
        "                    content = future.result()\n",
        "                    batch_contents.append(content)\n",
        "\n",
        "                yield batch_contents\n",
        "\n",
        "    def get_filenames(self, data_dir):\n",
        "        filenames = []\n",
        "        for root, dirs, files in os.walk(data_dir):\n",
        "            for file in files:\n",
        "                filenames.append(os.path.join(root, file))\n",
        "        return filenames\n",
        "\n",
        "    def read_file(self, filename):\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "\n",
        "    def save(self, fp):\n",
        "        self.tokenizer.save_pretrained(fp)\n",
        "\n",
        "def main(data_dir, batch_size=1000, vocab_size=50357, save=True, save_fp='tokenizer/adapted-tokenizer'):\n",
        "    bpe_trainer = BPETrainer()\n",
        "    bpe_trainer.train(data_dir=data_dir, batch_size=batch_size, vocab_size=vocab_size, save=save, save_fp=save_fp)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"aclImdb\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEo2vPhSZmHb"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw3FwMCkqtM2"
      },
      "source": [
        "The provided code defines two classes for tokenizing sentences using the Hugging Face Transformers library:\n",
        "\n",
        "1. **`DataTokenizer` Abstract Base Class (ABC):**\n",
        "\n",
        "    This abstract class serves as a base for all tokenization classes and defines the interface for tokenizing sentences. It includes an abstract method `tokenize` that must be implemented by any derived class. This method takes an input sentence and returns tokenized input IDs.\n",
        "\n",
        "2. **`EnglishDataTokenizer` Class:**\n",
        "\n",
        "    This class is an implementation of the `DataTokenizer` abstract class specifically tailored for tokenizing English sentences. It initializes a tokenizer from a given path or name of a pre-trained tokenizer using the Hugging Face `AutoTokenizer`. The `tokenize` method of this class tokenizes a given sentence, ensuring it's padded to a maximum length specified during initialization. The tokenized input IDs are returned.\n",
        "\n",
        "\n",
        "**Note:** In GPT models, the text is usually generated without specific markers as the beginning of a sequence, and there may not be a dedicated token for unknown words or subwords because subword tokenization is often capable of handling most text. Keep in mind that the absence of certain special tokens doesn't limit the model's ability to generate coherent text. Instead, it focuses on the language generation aspect. If you have specific requirements for using additional special tokens, you might need to adapt or extend the tokenizer accordingly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, logging\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "# Set the logging level for transformers library to ERROR\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "class DataTokenizer(metaclass=ABCMeta):\n",
        "    \"\"\"\n",
        "    A wrapper class for tokenizing sentences using the Hugging Face transformers library.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def tokenize(self, sentence):\n",
        "        \"\"\"\n",
        "        Tokenizes a sentence and converts it to token IDs.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): The input sentence to be tokenized.\n",
        "\n",
        "        Returns:\n",
        "            List[int]: List of token IDs.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def detokenize(self, input_ids):\n",
        "        \"\"\"\n",
        "        Converts token IDs back to a sentence.\n",
        "\n",
        "        Args:\n",
        "            input_ids (List[int]): List of token IDs.\n",
        "\n",
        "        Returns:\n",
        "            str: Detokenized sentence.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class EnglishDataTokenizer(DataTokenizer):\n",
        "    def __init__(self, tokenizer_path, sequence_length, training=True):\n",
        "        \"\"\"\n",
        "        Initializes the EnglishDataTokenizer.\n",
        "\n",
        "        Args:\n",
        "            tokenizer_path (str): The name or path of the pre-trained tokenizer.\n",
        "            sequence_length (int): The maximum length of tokenized sequences.\n",
        "            training (bool): Whether the tokenizer is used for training or not.\n",
        "        \"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, pad_token=\"[PAD]\")\n",
        "        self.sequence_length = sequence_length + 1 if training else sequence_length\n",
        "        self.padding = 'max_length' if training else 'do_not_pad'\n",
        "        self.truncation = True if training else False\n",
        "        self.training = training\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        if self.training:\n",
        "            sentence += '<|endoftext|>'\n",
        "        tokenized_sentence = self.tokenizer(sentence,\n",
        "                                            padding=self.padding,\n",
        "                                            max_length=self.sequence_length,\n",
        "                                            truncation=self.truncation,\n",
        "                                            return_tensors=\"np\")\n",
        "        return tokenized_sentence['input_ids'][0]\n",
        "\n",
        "    def detokenize(self, input_ids):\n",
        "        return self.tokenizer.decode(input_ids)\n",
        "\n",
        "    def convert_to_tokens(self, sentence):\n",
        "        if self.training:\n",
        "            sentence += '<|endoftext|>'\n",
        "            tokenized_sentence = self.tokenizer.tokenize(sentence,\n",
        "                                                         padding=self.padding,\n",
        "                                                         max_length=self.sequence_length,\n",
        "                                                         truncation=self.truncation)\n",
        "\n",
        "        return tokenized_sentence\n",
        "\n",
        "    def convert_to_ids(self, tokens):\n",
        "        return self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    def tokenize2(self, sentence):\n",
        "        tokens = self.convert_to_tokens(sentence)\n",
        "        input_ids = self.convert_to_ids(tokens)\n",
        "\n",
        "        return input_ids"
      ],
      "metadata": {
        "id": "44oEyVQ99A_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_path = \"tokenizer/adapted-tokenizer\"\n",
        "max_length = 5\n",
        "\n",
        "# Create a Tokenizer instance\n",
        "tokenizer = EnglishDataTokenizer(tokenizer_path, max_length)\n",
        "\n",
        "# Test sentence\n",
        "test_sentence = \"This movie is\"\n",
        "tokenized_input_ids = tokenizer.tokenize(test_sentence)\n",
        "sentence  = tokenizer.detokenize(tokenized_input_ids)\n",
        "tokenized_input_ids2 = tokenizer.tokenize2(test_sentence)\n",
        "sentence2  = tokenizer.detokenize(tokenized_input_ids2)\n",
        "\n",
        "# Print the tokenized input IDs\n",
        "print(\"Tokenized Input IDs:\", tokenized_input_ids)\n",
        "print(\"Sentence:\", sentence)\n",
        "\n",
        "print(\"Tokenized Input IDs2:\", tokenized_input_ids2)\n",
        "print(\"Sentence2:\", sentence2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdZ-zIfF5I7R",
        "outputId": "f671bee9-fb2a-41c0-be09-cf6fc8345f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Input IDs: [  751   360   296     0 50357 50357]\n",
            "Sentence: This movie is<|endoftext|>[PAD][PAD]\n",
            "Tokenized Input IDs2: [751, 360, 296, 0, 50357, 50357]\n",
            "Sentence2: This movie is<|endoftext|>[PAD][PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGUyRNtCTdNZ"
      },
      "source": [
        "## Data Stremer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzjgdOzetqLX"
      },
      "source": [
        "**Note:** First of all you might be wondering why I didn't use `tf.data` here. The reason is that you can't combine it and the tokenizer from the Hugging Face library into a single pipeline. There is one solution that can be used but I will not go into it, because it is ineffective. That's why I'm building a pipeline from scratch that follows some useful practices found in `tf.data`. It should be noted that this code uses parallelization, which may have a subtle effect (and may even slow down) in the case of small corps like ours.\n",
        "\n",
        "This code defines a data streaming framework for loading and iterating over tokenized data sequences in batches. It includes an abstract base class `DataStreamer` that outlines the required methods and their functionalities. It also provides a concrete implementation called `EnglishDataStreamer` tailored for English data.\n",
        "\n",
        "The `DataStreamer` ABC includes the following methods:\n",
        "- `__len__()`: Calculates the number of batches in the dataset.\n",
        "- `_fetch_to_buffer()`: Fetches data from files into an internal buffer using parallelization.\n",
        "- `__iter__()`: Returns the iterator object for iterating over batches of data.\n",
        "- `_start_fetching()`: Starts the background fetching of data.\n",
        "- `__next__()`: Returns the next batch of tokenized sequences.\n",
        "- `_get_filenames(data_dir)`: Retrieves a list of file paths from subdirectories.\n",
        "- `_reset()`: Resets the buffer index and fetches new data if necessary.\n",
        "\n",
        "The `EnglishDataStreamer` class, which inherits from `DataStreamer`, implements the specifics of the abstract methods and adds functionality for handling English data. It takes various arguments like the root directory of data files, tokenizer information, batch size, maximum sequence length, and more.\n",
        "\n",
        "Key features of `EnglishDataStreamer`:\n",
        "- It maintains an internal buffer to efficiently load and manage tokenized sequences.\n",
        "- Parallelization is used to fetch and process data from files into the buffer.\n",
        "- The `__next__()` method prepares and returns batches of input sequences and their corresponding labels, suitable for training a language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2AN2xpJMfgi"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import os, re\n",
        "import threading\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "class DataStreamer(metaclass=ABCMeta):\n",
        "    \"\"\"\n",
        "    A data streamer for loading and iterating over tokenized data sequences in batches.\n",
        "\n",
        "    Methods:\n",
        "        __len__(): Returns the number of batches in the dataset.\n",
        "        _fetch_to_buffer(): Fetches data from files into the buffer.\n",
        "        __iter__(): Returns the iterator object.\n",
        "        __next__(): Returns the next batch of tokenized sequences.\n",
        "        _get_filenames(data_dir): Retrieves a list of file paths from subdirectories.\n",
        "        _reset(): Resets the buffer index and fetches new data if necessary.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Calculates the number of batches in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of batches.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def fetch_to_buffer(self):\n",
        "        \"\"\"\n",
        "        Fetches data from files into the internal buffer using parallelization.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Returns the iterator object for iterating over batches of data.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __next__(self):\n",
        "        \"\"\"\n",
        "        Retrieves the next batch of sentences from the buffer.\n",
        "\n",
        "        Returns:\n",
        "            list: Batch of sentences.\n",
        "        Raises:\n",
        "            StopIteration: If there is no more data to retrieve.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_filenames(self):\n",
        "        \"\"\"\n",
        "        Retrieves a list of file paths from subdirectories within the given root directory.\n",
        "\n",
        "        Returns:\n",
        "            list: List of file paths.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Resets the buffer index and fetches new data if necessary.\n",
        "        \"\"\"\n",
        "\n",
        "class EnglishDataStreamer(DataStreamer):\n",
        "    \"\"\"\n",
        "    A specialized implementation of DataStreamer for handling English language data.\n",
        "\n",
        "    This class extends the functionality of the DataStreamer class to specifically\n",
        "    handle English language data. It provides methods for loading and processing\n",
        "    English text data for further use in natural language processing tasks.\n",
        "\n",
        "    Args:\n",
        "        config (object): Configuration object containing relevant parameters.\n",
        "\n",
        "    Attributes:\n",
        "        buffer_idx (int): Index pointing to the current position in the buffer.\n",
        "        buffer (list): List to hold fetched data for efficient batching.\n",
        "        ptr (int): Pointer for data processing within the buffer.\n",
        "        flag (bool): Flag indicating the status of data fetching.\n",
        "        dataset_type (str): Either \"train\" or \"valid\".\n",
        "        data_dir (str): dataset diractory.\n",
        "        buffer_size (int): Size of the buffer for holding fetched data.\n",
        "        batch_size (int): Size of each batch of data to be processed.\n",
        "        tokenizer_path (str): Path to the tokenizer used for text processing.\n",
        "        sequence_length (int): Maximum length of sequences after tokenization.\n",
        "        shuffle (bool): Flag indicating whether to shuffle the data.\n",
        "        lower_case (bool): Flag indicating whether to convert text to lowercase.\n",
        "        remove_punctuation (bool): to remove punctuation or not.\n",
        "        random_state: Random state generator for reproducibility.\n",
        "        filenames (list): List of file names containing the data.\n",
        "        tokenizer: Instance of the EnglishDataTokenizer for text processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, dataset_type):\n",
        "        assert config.buffer_size >= config.batch_size, \"buffer_size should be equal or greater than batch_size\"\n",
        "\n",
        "        # Initialize attributes\n",
        "        self.buffer_idx = 0\n",
        "        self.buffer = []\n",
        "        self.ptr = 0\n",
        "        self.flag = False\n",
        "        self.dataset_type = dataset_type\n",
        "        self.data_dir = config.data_dir\n",
        "        self.buffer_size = config.buffer_size\n",
        "        self.batch_size = config.batch_size\n",
        "        self.tokenizer_path = config.tokenizer_path\n",
        "        self.sequence_length = config.sequence_length\n",
        "        self.shuffle = config.shuffle\n",
        "        self.lower_case = config.lower_case\n",
        "        self.remove_punctuation = config.remove_punctuation\n",
        "        self.random_state = np.random.RandomState(config.seed)\n",
        "        self.filenames = self.get_filenames()\n",
        "        self.tokenizer = EnglishDataTokenizer(config.tokenizer_path, config.sequence_length)\n",
        "        self.fetch_to_buffer()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.filenames) / self.batch_size))\n",
        "\n",
        "    def fetch_to_buffer(self):\n",
        "        #print(\"Fetching...\")\n",
        "        self.buffer = self.buffer[self.ptr:]\n",
        "        self.ptr = 0\n",
        "\n",
        "        def custom_standardization(sentence):\n",
        "            \"\"\" Remove html line-break tags and lowercasing \"\"\"\n",
        "            sentence = re.sub(\"<br />\", \" \", sentence).strip()\n",
        "            if self.lower_case:\n",
        "                sentence = sentence.lower()\n",
        "            # remove punctuation\n",
        "            if self.remove_punctuation:\n",
        "                sentence = re.sub(f\"[{re.escape(string.punctuation)}]\", r\" \", sentence)\n",
        "            return sentence\n",
        "\n",
        "        def process_data(filename):\n",
        "            \"\"\" Read the data and perform the necessary processing and conversion operations \"\"\"\n",
        "            with open(filename, \"r\") as file:\n",
        "                sentence = file.readline()\n",
        "                sentence = custom_standardization(sentence)\n",
        "                try:\n",
        "                    tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
        "                except:\n",
        "                    print(sentence)\n",
        "                    raise ValueError(\"MyError\")\n",
        "                return tokenized_sentence\n",
        "\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            sentences = sentences = list(executor.map(process_data, self.filenames[self.buffer_idx:self.buffer_idx + self.buffer_size]))\n",
        "\n",
        "        self.buffer.extend(sentences)\n",
        "        self.buffer_idx += self.buffer_size\n",
        "\n",
        "        if self.shuffle:\n",
        "            self.random_state.shuffle(self.buffer)\n",
        "\n",
        "        #print(\"Buffer size:\", len(self.buffer))\n",
        "        #print(\"Buffer elements:\", self.buffer)\n",
        "        #print(\"Fetching is complete\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"\n",
        "        Retrieves the next batch of sentences from the buffer.\n",
        "\n",
        "        Returns:\n",
        "            list: Batch of sentences.\n",
        "        Raises:\n",
        "            StopIteration: If there is no more data to retrieve.\n",
        "        \"\"\"\n",
        "        def prepare_lm_inputs_labels(batch):\n",
        "            \"\"\"\n",
        "            Shift word sequences by 1 position so that the target for position (i) is\n",
        "            word at position (i+1). The model will use all words up till position (i)\n",
        "            to predict the next word.\n",
        "            \"\"\"\n",
        "            batch = tf.convert_to_tensor(batch)\n",
        "            x = batch[:, :-1]\n",
        "            y = batch[:, 1:]\n",
        "            return [x, y]\n",
        "\n",
        "        if self.flag:\n",
        "            self.flag = False\n",
        "            self.reset()\n",
        "            raise StopIteration\n",
        "\n",
        "        if self.ptr + self.batch_size > self.buffer_size:\n",
        "            self.fetch_to_buffer()\n",
        "\n",
        "        batch = self.buffer[self.ptr:self.ptr + self.batch_size]\n",
        "        self.ptr += self.batch_size\n",
        "\n",
        "        if len(batch) < self.batch_size and self.buffer_idx >= len(self.filenames):\n",
        "            self.flag = True\n",
        "\n",
        "        return prepare_lm_inputs_labels(batch)\n",
        "\n",
        "\n",
        "\n",
        "    def get_filenames(self):\n",
        "        if self.dataset_type not in [\"train\", \"valid\"]:\n",
        "            raise ValueError(\"Invalid dataset type. Choose 'train' or 'valid'.\")\n",
        "\n",
        "        base_dir = self.data_dir\n",
        "\n",
        "        if self.dataset_type == \"train\":\n",
        "            dataset_dirs = [\"train\", \"test\"]\n",
        "        else:\n",
        "            dataset_dirs = [self.dataset_type]\n",
        "\n",
        "        all_files = []\n",
        "\n",
        "        for dataset_dir in dataset_dirs:\n",
        "            dataset_dir = os.path.join(base_dir, dataset_dir)\n",
        "            pos_dir = os.path.join(dataset_dir, \"pos\")\n",
        "            neg_dir = os.path.join(dataset_dir, \"neg\")\n",
        "\n",
        "            pos_files = [os.path.join(pos_dir, filename) for filename in os.listdir(pos_dir)]\n",
        "            neg_files = [os.path.join(neg_dir, filename) for filename in os.listdir(neg_dir)]\n",
        "\n",
        "            # Combine positive and negative file lists\n",
        "            all_files.extend(pos_files)\n",
        "            all_files.extend(neg_files)\n",
        "\n",
        "        return all_files\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer_idx = 0\n",
        "        self.fetch_to_buffer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81k0ZgIRxVoC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "optK4GesrJk3"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  Here we create a dicactory that matches the dicactory structure we have and put only 7 files in it,\n",
        "#  each with the letter 't' alongside the corresponding file number. Ex: t1 for the first one,\n",
        "#  t2 for the second one, and so on. Then we run tests to make sure the streamer works as expected.\n",
        "\n",
        "\n",
        "class ConfigDr:\n",
        "    def __init__(self):\n",
        "        self.tokenizer_path = \"tokenizer/adapted-tokenizer\"\n",
        "        self.data_dir = 'dummy_data'\n",
        "        self.checkpoint_filepath = 'tmp/checkpoint'\n",
        "        self.shuffle = True\n",
        "        self.lower_case = True\n",
        "        self.sequence_length = 3\n",
        "        self.buffer_size = 4\n",
        "        self.batch_size = 2\n",
        "        self.seed = 2024\n",
        "        self.remove_punctuation = True\n",
        "\n",
        "config_dr = ConfigDr()\n",
        "data_streamer = EnglishDataStreamer(config_dr, 'train')\n",
        "\n",
        "for _ in range(2):\n",
        "    for batch_num, batch_data in enumerate(data_streamer):\n",
        "        print(f\"Batch {batch_num + 1}:\")\n",
        "        #print(batch_data[0].shape)\n",
        "        #print((batch_data))\n",
        "        #break\n",
        "        print(\"\\n----------------------\\n\")\n",
        "\n",
        "    print(\"\\nEnd of epoch \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAG5-LiqtnF3"
      },
      "source": [
        "## GPT Building Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQKBg7Plix3r"
      },
      "source": [
        "Most of the following components in this section were discussed earlier in [Neural-Machine-Translator](https://github.com/AliHaiderAhmad001/Neural-Machine-Translator/blob/main/README.md) repo (in the demo folder). Almost the only difference is the removal of the encoder and the modification of the decoder a bit. So we won't talk about the next layers anymore.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P275Ngt2uC1U"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1gLjOw0hFN_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class PositionalEmbeddings(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    PositionalEmbeddings layer.\n",
        "\n",
        "    This layer generates positional embeddings based on input IDs.\n",
        "    It uses an Embedding layer to map position IDs to position embeddings.\n",
        "\n",
        "    Args:\n",
        "        config (object): Configuration object containing parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, name = None, **kwargs):\n",
        "        super(PositionalEmbeddings, self).__init__(name=name, **kwargs)\n",
        "        self.positional_embeddings = tf.keras.layers.Embedding(\n",
        "            input_dim=config.sequence_length, output_dim=config.hidden_size\n",
        "        )\n",
        "\n",
        "    def call(self, input_ids):\n",
        "        \"\"\"\n",
        "        Generate positional embeddings.\n",
        "\n",
        "        Args:\n",
        "            input_ids (tf.Tensor): Input tensor containing token IDs.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Positional embeddings tensor of shape (batch_size, seq_length, hidden_size).\n",
        "        \"\"\"\n",
        "        seq_length = tf.shape(input_ids)[1]\n",
        "        position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
        "        position_embeddings = self.positional_embeddings(position_ids)\n",
        "        return position_embeddings\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Get the layer configuration.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing the layer configuration.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"positional_embeddings\": self.positional_embeddings,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class Embeddings(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Embeddings layer.\n",
        "\n",
        "    This layer combines token embeddings with positional embeddings to create the final embeddings.\n",
        "\n",
        "    Args:\n",
        "        config (object): Configuration object containing parameters.\n",
        "\n",
        "    Attributes:\n",
        "        token_embeddings (tf.keras.layers.Embedding): Token embedding layer.\n",
        "        dropout (tf.keras.layers.Dropout): Dropout layer for regularization.\n",
        "        norm (tf.keras.layers.LayerNormalization): Layer normalization for normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, name = None,  **kwargs):\n",
        "        super(Embeddings, self).__init__(name=name, **kwargs)\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            input_dim= config.vocab_size, output_dim=config.hidden_size\n",
        "        )\n",
        "        self.PositionalInfo = PositionalEmbeddings(config)\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, input_ids, training=False):\n",
        "        \"\"\"\n",
        "        Generate embeddings for input IDs.\n",
        "\n",
        "        Args:\n",
        "            input_ids (tf.Tensor): Input tensor containing token IDs.\n",
        "            training (bool, optional): Whether the model is in training mode. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Embeddings tensor of shape (batch_size, seq_length, hidden_size).\n",
        "        \"\"\"\n",
        "        positional_info = self.PositionalInfo(input_ids)\n",
        "        x = self.token_embeddings(input_ids)\n",
        "        x += positional_info\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        Computes the mask for the inputs.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Input tensor.\n",
        "            mask (tf.Tensor, optional): Mask tensor. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Computed mask tensor.\n",
        "        \"\"\"\n",
        "        return tf.math.not_equal(inputs, 50357)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Get the layer configuration.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing the layer configuration.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"token_embeddings\": self.token_embeddings,\n",
        "            \"PositionalInfo\": self.PositionalInfo,\n",
        "            \"dropout\": self.dropout,\n",
        "            \"norm\": self.norm,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3MJ6_ZHubMW"
      },
      "source": [
        "### Autoregressive Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWRR6YaKugHb"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Attention head implementation.\n",
        "\n",
        "    Args:\n",
        "        head_dim: Dimensionality of the attention head.\n",
        "\n",
        "    Attributes:\n",
        "        head_dim: Dimensionality of the attention head.\n",
        "        query_weights: Dense layer for query projection.\n",
        "        key_weights: Dense layer for key projection.\n",
        "        value_weights: Dense layer for value projection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_dim, name = None, **kwargs):\n",
        "        super(AttentionHead, self).__init__(name=name, **kwargs)\n",
        "        self.supports_masking = True  # Enable masking support\n",
        "        self.head_dim = head_dim\n",
        "        self.query_weights = tf.keras.layers.Dense(head_dim)\n",
        "        self.key_weights = tf.keras.layers.Dense(head_dim)\n",
        "        self.value_weights = tf.keras.layers.Dense(head_dim)\n",
        "\n",
        "    def call(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Applies attention mechanism to the input query, key, and value tensors.\n",
        "\n",
        "        Args:\n",
        "            query: Query tensor.\n",
        "            key: Key tensor.\n",
        "            value: Value tensor.\n",
        "            mask: Optional mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            Updated value embeddings after applying attention mechanism.\n",
        "        \"\"\"\n",
        "        query = self.query_weights(query)\n",
        "        key = self.key_weights(key)\n",
        "        value = self.value_weights(value)\n",
        "\n",
        "        att_scores = tf.matmul(query, tf.transpose(key, perm=[0, 2, 1])) / tf.math.sqrt(tf.cast(tf.shape(query)[-1], tf.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = tf.cast(mask, dtype=tf.bool)\n",
        "            att_scores = tf.where(mask, att_scores, tf.constant(-1e9, dtype=att_scores.dtype))\n",
        "\n",
        "        att_weights = tf.nn.softmax(att_scores, axis=-1)\n",
        "        n_value = tf.matmul(att_weights, value)\n",
        "\n",
        "        return n_value\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration of the attention head layer.\n",
        "\n",
        "        Returns:\n",
        "            Configuration dictionary.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"head_dim\": self.head_dim,\n",
        "            \"query_weights\": self.query_weights,\n",
        "            \"key_weights\": self.key_weights,\n",
        "            \"value_weights\": self.value_weights,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class MultiHead_Attention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi-head attention layer implementation.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing hyperparameters.\n",
        "\n",
        "    Attributes:\n",
        "        supports_masking: Boolean indicating if the layer supports masking.\n",
        "        hidden_size: Dimensionality of the hidden state.\n",
        "        num_heads: Number of attention heads.\n",
        "        head_dim: Dimensionality of each attention head.\n",
        "        attention_heads: List of AttentionHead layers.\n",
        "        fc: Fully connected layer for final projection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, name=None, **kwargs):\n",
        "        super(MultiHead_Attention, self).__init__(name=name, **kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.hidden_size // config.num_heads\n",
        "        self.attention_heads = [AttentionHead(self.head_dim) for _ in range(self.num_heads)]\n",
        "        self.fc = tf.keras.layers.Dense(config.hidden_size)\n",
        "\n",
        "    def call(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Applies multi-head attention mechanism to the input query, key, and value tensors.\n",
        "\n",
        "        Args:\n",
        "            query: Query tensor.\n",
        "            key: Key tensor.\n",
        "            value: Value tensor.\n",
        "            mask: Optional mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            Updated hidden state after applying multi-head attention mechanism.\n",
        "        \"\"\"\n",
        "        attention_outputs = [attention_head(query, key, value, mask=mask) for attention_head in self.attention_heads]\n",
        "        hidden_state = tf.concat(attention_outputs, axis=-1)\n",
        "        hidden_state = self.fc(hidden_state)\n",
        "        return hidden_state\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration of the multi-head attention layer.\n",
        "\n",
        "        Returns:\n",
        "            Configuration dictionary.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"hidden_size\": self.hidden_size,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"head_dim\": self.head_dim,\n",
        "            \"attention_heads\": self.attention_heads,\n",
        "            \"fc\": self.fc,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkFuo4HauMFS"
      },
      "source": [
        "### The Feed-Forward Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVMcGpDou2H3"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Feed-forward layer implementation.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing hyperparameters.\n",
        "\n",
        "    Attributes:\n",
        "        supports_masking: Boolean indicating if the layer supports masking.\n",
        "        fc1: First dense layer.\n",
        "        fc2: Second dense layer.\n",
        "        dropout: Dropout layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, name=None, **kwargs):\n",
        "        super(FeedForward, self).__init__(name=name, **kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.fc1 = tf.keras.layers.Dense(config.intermediate_fc_size, activation=tf.keras.activations.gelu)\n",
        "        self.fc2 = tf.keras.layers.Dense(config.hidden_size)\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, hidden_state, training=False):\n",
        "        \"\"\"\n",
        "        Applies feed-forward transformation to the input hidden state.\n",
        "\n",
        "        Args:\n",
        "            hidden_state: Hidden state tensor (batch_size, sequence_length, hidden_size).\n",
        "            training: Boolean indicating whether the model is in training mode or inference mode.\n",
        "\n",
        "        Returns:\n",
        "            Updated hidden state after applying feed-forward transformation.\n",
        "        \"\"\"\n",
        "        hidden_state = self.fc1(hidden_state)\n",
        "        hidden_state = self.dropout(hidden_state, training=training)\n",
        "        hidden_state = self.fc2(hidden_state)\n",
        "        return hidden_state\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration of the feed-forward layer.\n",
        "\n",
        "        Returns:\n",
        "            Configuration dictionary.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"fc1\": self.fc1,\n",
        "            \"fc2\": self.fc2,\n",
        "            \"dropout\": self.dropout,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA81BOhSu5VV"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mAXiSfGmTho"
      },
      "source": [
        "The main difference in the decoder in GPT from the decoder in transformer model is that there is no longer an encoder, so there are no more inputs to the decoder from the encoder, and thus there is no need for a second attention layer. That's it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7yJLdxSu68v"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Decoder layer implementation.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing hyperparameters.\n",
        "\n",
        "    Attributes:\n",
        "        masked_multihead_attention: Masked multi-head attention layer.\n",
        "        norm1: Layer normalization layer.\n",
        "        norm2: Layer normalization layer.\n",
        "        feed_forward: Feed-forward layer.\n",
        "        dropout: Dropout layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, name=None, **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "        self.masked_multihead_attention = MultiHead_Attention(config)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, hidden_state, mask=None, training=False):\n",
        "        \"\"\"\n",
        "        Applies the decoder layer to the input hidden state.\n",
        "\n",
        "        Args:\n",
        "            hidden_state: Hidden state tensor.\n",
        "            mask: mask tensor.\n",
        "            training: Boolean indicating if the model is in training mode.\n",
        "\n",
        "        Returns:\n",
        "            Updated hidden state after applying the decoder layer.\n",
        "        \"\"\"\n",
        "        causal_mask = self.get_causal_attention_mask(hidden_state)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output = self.masked_multihead_attention(hidden_state, hidden_state, hidden_state, mask=causal_mask)\n",
        "        attention_output = self.dropout(attention_output, training=training)\n",
        "        hidden_state = self.norm1(attention_output + hidden_state)\n",
        "\n",
        "        feed_forward_output = self.feed_forward(hidden_state)\n",
        "        feed_forward_output = self.dropout(feed_forward_output, training=training)\n",
        "        hidden_state = self.norm2(feed_forward_output + hidden_state)\n",
        "\n",
        "        return hidden_state\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        \"\"\"\n",
        "        Generates the causal attention mask.\n",
        "\n",
        "        Args:\n",
        "            inputs: Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Causal attention mask tensor.\n",
        "        \"\"\"\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=tf.int32)\n",
        "        mask = tf.reshape(mask, (1, sequence_length, sequence_length))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration of the decoder layer.\n",
        "\n",
        "        Returns:\n",
        "            Configuration dictionary.\n",
        "        \"\"\"\n",
        "        config = super(Decoder, self).get_config()\n",
        "        config.update({\n",
        "            \"masked_multihead_attention\": self.masked_multihead_attention,\n",
        "            \"norm1\": self.norm1,\n",
        "            \"norm2\": self.norm2,\n",
        "            \"feed_forward\": self.feed_forward,\n",
        "            \"dropout\": self.dropout,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztyJtx6az55u"
      },
      "source": [
        "### GPT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm5PsoFRoeuR"
      },
      "source": [
        "The `GPT` class is defined as a subclass of `tf.keras.Model`, which means it's designed to work with TensorFlow's Keras API for building and training neural networks. This class represents the core architecture of the GPT model and includes methods for creating the model's layers and performing the forward pass.\n",
        "\n",
        "- The `__init__` method initializes the GPT model. It takes a `config` argument, which is a configuration object containing hyperparameters for the model. Within this method:\n",
        "  - An embedding layer named `embed_layer` is created using the `Embeddings` class. This layer is used to convert input tokens into continuous vector representations.\n",
        "  - The `decoder` attribute is a list of `Decoder` layers. The number of decoder layers is determined by `config.num_blocks`.\n",
        "  - A dropout layer named `dropout` is included for regularization. The dropout rate is specified by `config.final_dropout_prob`.\n",
        "  - An output dense layer named `output_layer` is created to predict the probabilities of the next token in the sequence.\n",
        "\n",
        "- The `call` method defines the forward pass of the GPT model. Given input tokens (`inputs`), it:\n",
        "  - Passes the tokens through the embedding layer to obtain continuous embeddings.\n",
        "  - Sequentially feeds the embeddings through each decoder layer in the `decoder` list.\n",
        "  - Applies dropout to the decoder output for regularization.\n",
        "  - Passes the result through the output dense layer to obtain logits, which represent the predicted probabilities for each token in the vocabulary.\n",
        "  - Removes the mask from the logits since it's not needed in the loss function.\n",
        "\n",
        "- The `get_config` method returns a dictionary containing the configuration of the GPT model. It includes references to the `embed_layer`, `decoder` layers, `dropout` layer, and `output_layer`. This method is used for saving and loading model configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ur1Gw370Uks"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class GPT(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    GPT model implementation.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing model hyperparameters.\n",
        "\n",
        "    Attributes:\n",
        "        embed_layer: Embeddings layer for the decoder inputs.\n",
        "        decoder: List of decoder layers.\n",
        "        dropout: Dropout layer for regularization.\n",
        "        output_layer: Dense layer for output prediction.\n",
        "\n",
        "    Methods:\n",
        "        call: Forward pass of the GPT model.\n",
        "        get_config: Returns the configuration dictionary of the GPT model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, name=None, **kwargs):\n",
        "        super(GPT, self).__init__(name=name, **kwargs)\n",
        "        self.embed_layer = Embeddings(config, name=\"embeddings\")\n",
        "        self.decoder = [Decoder(config) for _ in range(config.num_blocks)]\n",
        "        self.dropout = tf.keras.layers.Dropout(config.final_dropout_prob)\n",
        "        self.output_layer = tf.keras.layers.Dense(config.vocab_size, name=\"output_layer\")\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the GPT model.\n",
        "\n",
        "        Args:\n",
        "            inputs: Input data.\n",
        "            training: Boolean flag indicating whether the model is in training mode or not.\n",
        "            mask: Optional mask for inputs.\n",
        "\n",
        "        Returns:\n",
        "            Output logits of the GPT model.\n",
        "        \"\"\"\n",
        "        x_dec = self.embed_layer(inputs)\n",
        "\n",
        "        for decoder_layer in self.decoder:\n",
        "            x_dec = decoder_layer(x_dec, training=training)\n",
        "\n",
        "        x_dec = self.dropout(x_dec, training=training)\n",
        "        x_logits = self.output_layer(x_dec)\n",
        "        # Remove the mask from the logits as it's not needed in the loss function\n",
        "        x_logits._keras_mask = None\n",
        "\n",
        "        return x_logits\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration dictionary of the GPT model.\n",
        "\n",
        "        Returns:\n",
        "            Configuration dictionary.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_layer\": self.embed_layer,\n",
        "            \"decoder\": self.decoder,\n",
        "            \"dropout\": self.dropout,\n",
        "            \"output_layer\": self.output_layer,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "infl7iXsR-nr"
      },
      "source": [
        "## Configration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXH1FdrOSBBp"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"\n",
        "    Configuration class for GPT-based text generation model.\n",
        "\n",
        "    Attributes:\n",
        "        tokenizer_path (str): The path to the pre-trained tokenizer.\n",
        "        data_dir (str): Directory containing the training data.\n",
        "        checkpoint_directory (str): Directory to save model and optimizer checkpoints.\n",
        "        model_weights_checkpoint_directory (str): Directory to save model wights checkpoints.\n",
        "        shuffle (bool): Whether to shuffle the training data.\n",
        "        lower_case (bool): Whether to convert text to lowercase during preprocessing.\n",
        "        remove_punctuation (bool): Whether to remove punctuation during preprocessing.\n",
        "        sequence_length (int): The maximum sequence length for input data.\n",
        "        buffer_size (int): Size of the data buffer for shuffling.\n",
        "        batch_size (int): Batch size for training.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "        vocab_size (int): Vocabulary size, including the PAD token.\n",
        "        hidden_size (int): Size of the hidden layers in the model.\n",
        "        intermediate_fc_size (int): Size of intermediate fully connected layers.\n",
        "        warmup_steps (int): Number of warm-up steps for learning rate scheduling.\n",
        "        max_learning_rate (float): Maximum learning rate for training.\n",
        "        hidden_dropout_prob (float): Dropout probability for hidden layers.\n",
        "        num_heads (int): Number of attention heads in the model.\n",
        "        final_dropout_prob (float): Dropout probability for the final layer.\n",
        "        num_blocks (int): Number of transformer blocks in the model.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        patience (int): Number of epochs with no improvement to trigger early stopping.\n",
        "        end_token (int): The identifier for the end-of-sentence symbol. When you train the tokenizer this identifier changes, so be careful.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer_path = \"tokenizer/adapted-tokenizer\"\n",
        "        self.data_dir = 'aclImdb'\n",
        "        self.checkpoint_directory = 'tmp/checkpoint'\n",
        "        self.model_weights_checkpoint_directory = 'tmp/weights_checkpoint'\n",
        "        self.shuffle = True\n",
        "        self.lower_case = True\n",
        "        self.remove_punctuation = True\n",
        "        self.sequence_length = 128\n",
        "        self.buffer_size = 42500\n",
        "        self.batch_size = 64\n",
        "        self.seed = 2023\n",
        "        self.vocab_size = 50357+1  # Because we have added the PAD token.\n",
        "        self.hidden_size = 256\n",
        "        self.intermediate_fc_size = self.hidden_size * 4\n",
        "        self.warmup_steps = 4000\n",
        "        self.max_learning_rate = 2.5e-4\n",
        "        self.total_number_of_training_samples = 42500\n",
        "        self.hidden_dropout_prob = 0.1\n",
        "        self.num_heads = 4\n",
        "        self.final_dropout_prob = 0.5\n",
        "        self.num_blocks = 2\n",
        "        self.num_epochs = 20\n",
        "        self.patience = 4\n",
        "        self.end_token = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4HRfrFyB3Nu"
      },
      "source": [
        "## End-to-end GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vlwtDEkB5zm"
      },
      "source": [
        "### LrSchedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_1wJDqYNEkp"
      },
      "source": [
        "According to the paper:\n",
        "\n",
        ">We used the Adam optimization with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WPyhFeSB5Cd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class LrSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"\n",
        "    Learning rate schedule for training a model.\n",
        "\n",
        "    This class implements a learning rate schedule that combines linear warmup\n",
        "    followed by a cosine annealing schedule. It is designed to be used as the\n",
        "    learning rate schedule for the optimizer during training.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing schedule hyperparameters.\n",
        "\n",
        "    Attributes:\n",
        "        warmup_steps: Number of warmup steps during which the learning rate increases linearly.\n",
        "        max_learning_rate: Maximum learning rate reached after warmup.\n",
        "        total_steps: Total number of training steps.\n",
        "        learning_rate_schedule: Learning rate schedule for warmup phase.\n",
        "        cosine_schedule: Learning rate schedule for cosine annealing phase.\n",
        "\n",
        "    Methods:\n",
        "        __call__: Returns the learning rate for a given training step.\n",
        "        get_config: Returns the configuration dictionary of the learning rate schedule.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(LrSchedule, self).__init__()\n",
        "        self.warmup_steps = config.warmup_steps\n",
        "        self.max_learning_rate = config.max_learning_rate\n",
        "        self.total_steps = config.num_epochs * (config.total_number_of_training_samples // config.batch_size)\n",
        "        self.learning_rate_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "            initial_learning_rate=0.0,\n",
        "            decay_steps=self.warmup_steps,\n",
        "            end_learning_rate=self.max_learning_rate,\n",
        "            power=1.0  # Linear warmup\n",
        "        )\n",
        "        self.cosine_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "            initial_learning_rate=self.max_learning_rate,\n",
        "            decay_steps=self.total_steps - self.warmup_steps\n",
        "        )\n",
        "\n",
        "    def __call__(self, step):\n",
        "        \"\"\"\n",
        "        Returns the learning rate for a given training step.\n",
        "\n",
        "        Args:\n",
        "            step: Training step.\n",
        "\n",
        "        Returns:\n",
        "            Learning rate for the given step.\n",
        "        \"\"\"\n",
        "        def learning_rate_fn(step):\n",
        "            if step < self.warmup_steps:\n",
        "                return self.learning_rate_schedule(step)\n",
        "            return self.cosine_schedule(step - self.warmup_steps)\n",
        "\n",
        "        return tf.cond(step < self.warmup_steps,\n",
        "                       lambda: self.learning_rate_schedule(step),\n",
        "                       lambda: self.cosine_schedule(step - self.warmup_steps))\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration dictionary of the learning rate schedule.\n",
        "\n",
        "        Returns:\n",
        "            Configuration dictionary.\n",
        "        \"\"\"\n",
        "        learning_rate_schedule_config = tf.keras.optimizers.schedules.serialize(self.learning_rate_schedule)\n",
        "        cosine_schedule_config = tf.keras.optimizers.schedules.serialize(self.cosine_schedule)\n",
        "\n",
        "        return {\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "            \"max_learning_rate\": self.max_learning_rate,\n",
        "            \"total_steps\": self.total_steps,\n",
        "            \"learning_rate_schedule_config\": learning_rate_schedule_config,\n",
        "            \"cosine_schedule_config\": cosine_schedule_config,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPFYywOBCSbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "bfca148d-5294-4129-d2a6-ee465c1aa49f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import numpy as np\\nimport matplotlib.pyplot as plt\\n\\nclass ConfigLR:\\n    def __init__(self):\\n        self.num_epochs = 100\\n        self.batch_size = 64\\n        self.warmup_steps = 2000\\n        self.max_learning_rate = 2.5e-4\\n\\nconfig = ConfigLR()\\n\\nlr = LrSchedule(config, 100000)\\noptimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\\n\\n# Evaluate the learning rate tensor element-wise and collect the values in a numpy array\\nlearning_rates = np.array([lr(step).numpy() for step in range(40000)], dtype=np.float32)\\n\\nplt.plot(learning_rates)\\nplt.ylabel('Learning Rate')\\nplt.xlabel('Train Step')\\nplt.show()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\"\"\"import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ConfigLR:\n",
        "    def __init__(self):\n",
        "        self.num_epochs = 100\n",
        "        self.batch_size = 64\n",
        "        self.warmup_steps = 2000\n",
        "        self.max_learning_rate = 2.5e-4\n",
        "\n",
        "config = ConfigLR()\n",
        "\n",
        "lr = LrSchedule(config, 100000)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# Evaluate the learning rate tensor element-wise and collect the values in a numpy array\n",
        "learning_rates = np.array([lr(step).numpy() for step in range(40000)], dtype=np.float32)\n",
        "\n",
        "plt.plot(learning_rates)\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')\n",
        "plt.show()\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whplZuQBCFUO"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the ID of the filler code I use is 50375, and this may change depending on the tokenizer used. This also changes if you train tokenize."
      ],
      "metadata": {
        "id": "hxjZbeaWlnjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFAsugN0CI6r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def loss_fn(label, pred):\n",
        "    \"\"\"\n",
        "    Computes the masked Sparse Categorical Cross Entropy (SCCE) loss between the predicted and target labels.\n",
        "\n",
        "    Args:\n",
        "        label: Target label tensor.\n",
        "        pred: Predicted logit tensor.\n",
        "\n",
        "    Returns:\n",
        "        Masked loss value.\n",
        "    \"\"\"\n",
        "    # Create a mask to ignore padded tokens\n",
        "    mask = label != 50357\n",
        "\n",
        "    # Use Sparse Categorical Cross Entropy loss with no reduction\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "    # Compute the loss without reducing, which will return a loss value for each token\n",
        "    loss = loss_object(label, pred)\n",
        "\n",
        "    # Apply the mask to ignore padded tokens in the loss calculation\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Compute the average loss over non-padded tokens\n",
        "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-t3cf_CJYH"
      },
      "source": [
        "### Metrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htslSJlWlD4N"
      },
      "source": [
        "Perplexity is a metric commonly used to evaluate the performance of language models, including generative models like GPT. Intuitively, perplexity means to be surprised. We measure how much the model is surprised by seeing new data. The lower the perplexity, the better the training is.\n",
        "\n",
        "Mathematically, perplexity is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Perplexity} = 2^{-\\frac{1}{N}\\sum_{i=1}^{N} \\log_2 P(w_i|w_1, w_2, ..., w_{i-1})},\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\(N\\) is the total number of words or tokens in the sequence.\n",
        "- \\(w_i\\) represents the \\(i\\)th word or token in the sequence.\n",
        "- \\(P(w_i|w_1, w_2, ..., w_{i-1})\\) is the predicted probability assigned by the language model to the \\(i\\)th word given the previous words.\n",
        "\n",
        "In essence, perplexity calculates the average negative log-likelihood of the true next word according to the model's predicted distribution. A lower perplexity indicates that the model assigns higher probabilities to the actual next words, meaning that the model's predictions align well with the true data distribution.\n",
        "\n",
        "Perplexity is usually used only to determine how well a model has learned the training set. Other metrics like BLEU, ROUGE etc., are used on the test set to measure test performance.\n",
        "\n",
        "**Here are the main cons**:\n",
        "\n",
        "1. **Measuring Confidence, Not Accuracy**: Perplexity measures the model's confidence in its predictions but doesn't directly assess the accuracy of those predictions. A model can have low perplexity while still producing incorrect or nonsensical outputs if it consistently assigns high probabilities to incorrect words. It doesn't guarantee real-world performance or understanding.\n",
        "\n",
        "2. **Apples-to-Apples Comparisons**: Comparing perplexity values across different datasets or models can be challenging due to various factors such as context lengths, vocabulary sizes, and model architectures. As you mentioned, a lower perplexity value on one dataset or with a specific configuration doesn't guarantee better overall performance or quality. Directly comparing perplexity values between different scenarios might not provide a clear indication of which model is truly better for a given task.\n",
        "\n",
        ">We should note that the metric applies specifically to classical language models (sometimes called autoregressive or causal language models) and is not well defined for masked language models like BERT (see summary of the models).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtkwYvrAU4ce"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    Custom metric for calculating perplexity.\n",
        "\n",
        "    Perplexity is a measure of how well a probability distribution or probability model predicts a sample.\n",
        "    It is commonly used in natural language processing tasks to evaluate the quality of language models.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): Name of the metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name='perplexity', **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the Perplexity metric.\n",
        "\n",
        "        Args:\n",
        "            name (str, optional): Name of the metric. Defaults to 'perplexity'.\n",
        "            **kwargs: Additional keyword arguments passed to the base class.\n",
        "        \"\"\"\n",
        "        super(Perplexity, self).__init__(name=name, **kwargs)\n",
        "        self.loss_sum = self.add_weight(name='loss_sum', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Updates the metric state based on true and predicted values.\n",
        "\n",
        "        Args:\n",
        "            y_true (tensor): True target values.\n",
        "            y_pred (tensor): Predicted values.\n",
        "            sample_weight (tensor, optional): Optional weighting of samples. Not used.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "        mask = y_true != 50357  # Assumes that 50357 is the PAD token ID.\n",
        "        # Apply the mask to ignore padded tokens in the loss calculation\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        self.loss_sum.assign_add(tf.reduce_sum(loss))\n",
        "        self.count.assign_add(tf.reduce_sum(tf.cast(mask, tf.float32)))\n",
        "\n",
        "    def result(self):\n",
        "        \"\"\"\n",
        "        Computes the final perplexity metric.\n",
        "\n",
        "        Returns:\n",
        "            tensor: The computed perplexity value.\n",
        "        \"\"\"\n",
        "        return tf.pow(2.0, self.loss_sum / self.count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWBqp9hsCU64"
      },
      "source": [
        "### Monitor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "\n",
        "\n",
        "def create_model_optimizer(config):\n",
        "    model = GPT(config)\n",
        "\n",
        "    # Create the learning rate schedule\n",
        "    lr = LrSchedule(config)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        lr,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.98,\n",
        "        epsilon=1e-9\n",
        "    )\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "def save_model_and_optimizer(model, optimizer, checkpoint_manager, epoch):\n",
        "    # Save the model and optimizer state\n",
        "    checkpoint_name = checkpoint_manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}: {}\".format(epoch, checkpoint_name))\n",
        "\n",
        "def save_model_weights_only(model, model_weights_manager, epoch):\n",
        "    # Save model weights only\n",
        "    model_weights_checkpoint_name = model_weights_manager.save()\n",
        "    print(\"Saved model weights for epoch {}: {}\".format(epoch, model_weights_checkpoint_name))\n",
        "\n",
        "def train_one_epoch(model, optimizer, train_gr, loss_fn, train_ppe_metric):\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x, training=True)\n",
        "            loss_value = loss_fn(y, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        train_ppe_metric.update_state(y, logits)\n",
        "        return loss_value\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_gr):\n",
        "        tr_loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, tr_loss_value)\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * config.batch_size))\n",
        "\n",
        "def evaluate_one_epoch(model, val_gr, loss_fn, val_ppe_metric):\n",
        "    @tf.function\n",
        "    def test_step(x, y):\n",
        "        logits = model(x, training=False)\n",
        "        val_ppe_metric.update_state(y, logits)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "        return loss_value\n",
        "\n",
        "    val_loss_value = 0.0\n",
        "    for x_batch_val, y_batch_val in val_gr:\n",
        "        val_loss_value += test_step(x_batch_val, y_batch_val)\n",
        "    return val_loss_value\n",
        "\n",
        "def main_training_loop(config, resume_training=False):\n",
        "    python_random.seed(config.seed)\n",
        "    tf.random.set_seed(config.seed)\n",
        "\n",
        "    train_gr = EnglishDataStreamer(config, 'train')\n",
        "    val_gr = EnglishDataStreamer(config, 'valid')\n",
        "    # Prepare the metrics.\n",
        "    train_ppe_metric = Perplexity()\n",
        "    val_ppe_metric = Perplexity()\n",
        "\n",
        "    # Create the model and optimizer\n",
        "    if resume_training:\n",
        "        model, optimizer = load_model_and_optimizer(config)\n",
        "    else:\n",
        "        model, optimizer = create_model_optimizer(config)\n",
        "\n",
        "    # Create a checkpoint for both the model and optimizer\n",
        "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
        "    checkpoint_directory = config.checkpoint_directory\n",
        "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_directory, max_to_keep=1)\n",
        "\n",
        "    # Create a separate checkpoint for model weights only\n",
        "    model_weights_checkpoint = tf.train.Checkpoint(model=model)\n",
        "    model_weights_checkpoint_directory = config.model_weights_checkpoint_directory\n",
        "    model_weights_manager = tf.train.CheckpointManager(model_weights_checkpoint, model_weights_checkpoint_directory, max_to_keep=1)\n",
        "\n",
        "    # Early stopping parameters\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience = config.patience\n",
        "    wait = 0\n",
        "\n",
        "    for epoch in range(1, config.num_epochs+1):\n",
        "        print(\"\\n##### Start of epoch %d #####\" % (epoch,))\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        train_one_epoch(model, optimizer, train_gr, loss_fn, train_ppe_metric)\n",
        "\n",
        "        # Display metrics at the end of each epoch.\n",
        "        train_ppe = train_ppe_metric.result()\n",
        "        print(\"Training perplexity over epoch: %.4f\" % (float(train_ppe),))\n",
        "\n",
        "        # Reset training metrics at the end of each epoch\n",
        "        train_ppe_metric.reset_states()\n",
        "\n",
        "        # Evaluation\n",
        "        val_loss_value = evaluate_one_epoch(model, val_gr, loss_fn, val_ppe_metric)\n",
        "        print(\"Training loss over epoch: %.4f\" % (val_loss_value,))\n",
        "\n",
        "        val_ppe = val_ppe_metric.result()\n",
        "        val_ppe_metric.reset_states()\n",
        "\n",
        "        print(\"Validation perplexity: %.4f\" % (val_ppe,))\n",
        "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "\n",
        "        # Early Stopping Check\n",
        "        if val_loss_value < best_val_loss:\n",
        "            best_val_loss = val_loss_value\n",
        "            wait = 0\n",
        "            # Save the model and optimizer\n",
        "            save_model_and_optimizer(model, optimizer, manager, epoch)\n",
        "\n",
        "            # Save model weights only\n",
        "            save_model_weights_only(model, model_weights_manager, epoch)\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping triggered. No improvement in validation loss.\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    main_training_loop(config, resume_training=False)\n"
      ],
      "metadata": {
        "id": "PO_V4I_AkJCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model"
      ],
      "metadata": {
        "id": "bUzG6ix6SzZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "config = Config()\n",
        "\n",
        "custom_objects = {\n",
        "    \"LrSchedule\": LrSchedule,\n",
        "    \"PositionalEmbeddings\": PositionalEmbeddings,\n",
        "    \"Embeddings\": Embeddings,\n",
        "    \"AttentionHead\": AttentionHead,\n",
        "    \"MultiHead_Attention\": MultiHead_Attention,\n",
        "    \"FeedForward\": FeedForward,\n",
        "    \"Decoder\": Decoder,\n",
        "    \"GPT\": GPT,\n",
        "    \"loss_fn\": loss_fn,\n",
        "    \"Perplexity\": Perplexity\n",
        "}\n",
        "\n",
        "# I did not use `load_model_and_optimizer` function because I have worked with a small dataset,\n",
        "# but I wrote it in case you wanted to train the model on a large data set,\n",
        "# and then you wanted to save the entire model or you wanted to resume training at a later time.\n",
        "# Because if you want to resume training at a later time, you must completely reload the Optimizer state and Model.\n",
        "# This function is well tested,\n",
        "# but there were warnings which I filtered out\n",
        "# because I don't think they affect the model, but I suggest you check further.\n",
        "# However, if you decide to use it to resume model training,\n",
        "# you will have to modify the training loop or create another loop to accommodate it.\n",
        "\n",
        "def load_model_and_optimizer(config):\n",
        "    # Create the model and optimizer from checkpoint\n",
        "    model, optimizer = create_model_optimizer(config)\n",
        "\n",
        "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "          # Restore the model and optimizer state\n",
        "          latest_checkpoint = tf.train.latest_checkpoint(config.checkpoint_directory)\n",
        "          if latest_checkpoint:\n",
        "              checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
        "              checkpoint_status = checkpoint.restore(latest_checkpoint)\n",
        "              checkpoint_status.expect_partial()  # Suppress warnings about incomplete restores\n",
        "              print(\"Restored model and optimizer from checkpoint successfully: {}\".format(latest_checkpoint))\n",
        "          else:\n",
        "              print(\"Checkpoint not found. Initializing from scratch.\")\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "# We will use this function in the inference process\n",
        "def load_model_weights_only(config):\n",
        "    # Create the model\n",
        "    model = GPT(config)\n",
        "\n",
        "    # Restore only the model weights\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(config.model_weights_checkpoint_directory)\n",
        "    if latest_checkpoint:\n",
        "        model_weights_checkpoint = tf.train.Checkpoint(model=model)\n",
        "        checkpoint_status = model_weights_checkpoint.restore(latest_checkpoint)\n",
        "        checkpoint_status.expect_partial()  # Suppress warnings about incomplete restores\n",
        "        print(\"Restored model weights from checkpoint successfully: {}\".format(latest_checkpoint))\n",
        "    else:\n",
        "        print(\"Model weights checkpoint not found. Initializing model from scratch.\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "5n_Y_hsqmEe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "261nJZDfCcrT"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhZsyXJ9kiL"
      },
      "source": [
        "Transforming the model's probabilistic predictions into textual form involves the use of a decoding process, which presents several distinct challenges specific to text generation:\n",
        "\n",
        " Decoding is an iterative process, demanding considerably more computational resources than the single forward pass typically used for input processing.\n",
        "\n",
        " The excellence and variety of the generated text are contingent upon the selection of the decoding technique and its corresponding hyperparameters.\n",
        "\n",
        "You may be wondering how the process of generating text using the model we trained will work. The main idea is: We start with a prompt like \"this movie is\" and use the model to predict the next token. Once we have determined the next token, we append it to the prompt and then use the new input sequence to generate another token. We do this until we have reached to a predefined condition. How to choose the next token is the core of the decoding process. We now discuss the most important decoding algorithms.\n",
        "\n",
        "**Note:**This type of text generation is often called conditional text generation, because it depends on the input prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkUk0bO2jMdt"
      },
      "source": [
        "### **Greedy Search Decoding: A Deterministic Text Generation Approach**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIjvv8FgjVs7"
      },
      "source": [
        "\n",
        "Greedy search decoding is a simple and commonly used technique for text generation with autoregressive language models like GPT-2. In greedy search decoding, you always choose the token with the highest probability as the next token in the sequence. This approach is straightforward and fast but might result in less diverse and sometimes repetitive generated text.\n",
        "\n",
        "The mathematical formula for greedy search decoding in text generation can be expressed as follows:\n",
        "\n",
        "Let:\n",
        "- \\(S\\) be the generated sequence of tokens.\n",
        "- \\(P\\) be the probability distribution over vocabulary for the next token given the current sequence \\(S\\).\n",
        "- \\(t\\) represent the time step in the generation process, starting from \\(t = 0\\) for the initial seed text.\n",
        "- \\(T\\) be the maximum sequence length allowed.\n",
        "\n",
        "The process of greedy search decoding involves iteratively selecting the token with the highest probability at each time step \\(t\\) and appending it to the sequence \\(S\\), subject to the constraint that the length of \\(S\\) does not exceed \\(T\\). The selection of the next token can be defined as:\n",
        "\n",
        "$$\n",
        "S(t) = argmax_{i} P(i|S)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(S(t)\\) is the token selected at time step \\(t\\).\n",
        "- \\(argmax\\) denotes selecting the token index that maximizes the probability.\n",
        "- \\(P(i|S)\\) is the conditional probability of token \\(i\\) given the current sequence \\(S\\).\n",
        "\n",
        "The process continues until the maximum sequence length \\(T\\) is reached. We also can depend on an end-of-sequence token (e.g., \\<eos\\>) is predicted, indicating the completion of the generated text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AJNPLSIjD2g"
      },
      "outputs": [],
      "source": [
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "class Sampler(metaclass=ABCMeta):\n",
        "    \"\"\"\n",
        "    Abstract base class for text generation samplers.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def decode(self, sentence):\n",
        "        \"\"\"\n",
        "        Decode a sentence based on the implemented sampling strategy.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): The input sentence or prompt.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated output sentence.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class GreedySampler(Sampler):\n",
        "    \"\"\"\n",
        "    Greedy sampling strategy for text generation using a transformer-based language model.\n",
        "\n",
        "    This sampler uses a greedy approach to generate text based on the predictions of a transformer-based\n",
        "    language model. It tokenizes an input prompt, iteratively generates tokens, and selects the token\n",
        "    with the highest probability at each step until the maximum sequence length is reached or an\n",
        "    end token (if provided) is predicted.\n",
        "\n",
        "    Args:\n",
        "        model: The transformer-based language model.\n",
        "        tokenizer_path (str): The name or path of the pre-trained tokenizer.\n",
        "        sequence_length (int): The maximum length of tokenized sequences.\n",
        "        end_token (int or None, optional): An optional token ID that signifies the end of decoding. If\n",
        "            provided, decoding will stop when this token is predicted. Defaults to None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer_path, sequence_length, end_token=0):\n",
        "        \"\"\"\n",
        "        Initialize the GreedySampler.\n",
        "\n",
        "        Args:\n",
        "            model: The transformer-based language model.\n",
        "            tokenizer_path (str): The name or path of the pre-trained tokenizer.\n",
        "            sequence_length (int): The maximum length of tokenized sequences.\n",
        "            end_token (int or None, optional): An optional token ID that signifies the end of decoding.\n",
        "                If provided, decoding will stop when this token is predicted. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = EnglishDataTokenizer(tokenizer_path, sequence_length, training=False)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.end_token = end_token\n",
        "\n",
        "    def decode(self, input_prompt):\n",
        "        \"\"\"\n",
        "        Decode a sentence based on a greedy sampling strategy.\n",
        "\n",
        "        Args:\n",
        "            input_prompt (str): The input sentence or prompt.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated output sentence.\n",
        "        \"\"\"\n",
        "        input_ids = np.reshape(self.tokenizer.tokenize(input_prompt), (1, -1))\n",
        "        for _ in range(self.sequence_length):\n",
        "            predictions = self.model(input_ids, training=False)\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "            input_ids = tf.concat([input_ids, predicted_id], axis=-1)\n",
        "            if self.end_token is not None and predicted_id == self.end_token:\n",
        "                break\n",
        "\n",
        "        generated_text = self.tokenizer.detokenize(input_ids.numpy()[0])\n",
        "\n",
        "        return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aem9GJNjdJM"
      },
      "source": [
        "### **Beam Search Decoding: Enhancing Text Generation Precision and Diversity**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdFYANxzjfbk"
      },
      "source": [
        "Beam search decoding is a popular technique for generating text using autoregressive language models like GPT-2. It's a variation of greedy decoding that explores multiple possible continuations of a sequence to improve the quality and diversity of generated text. Instead of selecting the most likely next token at each step, beam search keeps track of the top `k` candidates (where `k` is called the \"beam width\") and selects from them.\n",
        "\n",
        "As you can notice; The primary goal is to find the most likely sequence of words or tokens in a probabilistic model, such as a language model. In contrast to greedy search, which selects the most likely word at each step, beam search keeps multiple possibilities, including those that may have lower probabilities at intermediate steps but lead to higher probabilities overall.\n",
        "\n",
        "\n",
        "Here's how beam search decoding works for text generation:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - Start with an initial input or prompt, typically a few words or a sentence.\n",
        "   - Set a parameter `num_beams`, which determines how many alternative sequences (beams) to consider at each decoding step. A higher `num_beams` value explores more possibilities but increases computational complexity.\n",
        "\n",
        "2. **Tokenization**:\n",
        "   - Tokenize the initial input to obtain the initial token IDs.\n",
        "\n",
        "3. **Decoding Loop**:\n",
        "   - Initialize `num_beams` sequences, each starting with the same initial tokens.\n",
        "   - At each decoding step:\n",
        "     - For each of the `num_beams` sequences:\n",
        "       - Generate the next token by sampling from the probability distribution over the vocabulary based on the current sequence's context.\n",
        "       - Extend each sequence with the generated token.\n",
        "       - Calculate the score (log probability) for each extended sequence.\n",
        "     - Select the top `num_beams` sequences with the highest scores.\n",
        "   - Repeat the decoding step until a stopping condition is met:\n",
        "     - The stopping condition may include reaching a maximum sequence length, generating a predefined end token (e.g., a period for sentence generation), or producing a certain number of sequences.\n",
        "\n",
        "4. **Output Selection**:\n",
        "   - After decoding is complete, you have `num_beams` candidate sequences.\n",
        "   - Choose the sequence with the highest cumulative score (log probability) as the final generated text.\n",
        "\n",
        "Here are some key points to understand about beam search decoding:\n",
        "\n",
        "- **Diverse Outputs**: Beam search allows for diversity in the generated text by exploring multiple hypotheses (beams) simultaneously. Different beams may produce variations of the same text, offering options for diverse outputs.\n",
        "\n",
        "- **Trade-Off**: The choice of `num_beams` is a trade-off between diversity and computational cost. Larger `num_beams` values increase diversity but also increase computation time.\n",
        "\n",
        "- **Prominent Use Cases**: Beam search is commonly used in machine translation, text summarization, and text generation tasks where generating coherent and contextually relevant text is crucial.\n",
        "\n",
        "- **Refinement**: To further improve text generation, techniques like length normalization and nucleus sampling can be combined with beam search.\n",
        "\n",
        "- **Beam Search Variants**: There are variants of beam search, such as diverse beam search, which aim to produce more diverse outputs by encouraging dissimilarity between beams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI2aJ2vmW-Bd"
      },
      "outputs": [],
      "source": [
        "class BeamSearchSampler(Sampler):\n",
        "    def __init__(self, model, tokenizer, sequence_length, beam_width=2, end_token=0):\n",
        "        \"\"\"\n",
        "        Initialize the BeamSearchSampler.\n",
        "\n",
        "        Args:\n",
        "            model: The language model used for text generation.\n",
        "            tokenizer: The tokenizer used for tokenizing text.\n",
        "            sequence_length (int): The maximum sequence length for generated text.\n",
        "            beam_width (int): Width of the beam search (number of beams). Default is 2.\n",
        "            end_token (int or None): The token indicating the end of a sequence, if applicable. Default is None.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = EnglishDataTokenizer(tokenizer_path, sequence_length, training=False)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.beam_width = beam_width\n",
        "        self.end_token = end_token\n",
        "\n",
        "    def decode(self, input_prompt):\n",
        "        \"\"\"\n",
        "        Decode a text sequence based on beam search.\n",
        "\n",
        "        Args:\n",
        "            input_prompt (str): The input text or prompt.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated output text.\n",
        "        \"\"\"\n",
        "        input_ids = np.reshape(self.tokenizer.tokenize(input_prompt), (1, -1))\n",
        "        iterations = self.sequence_length - input_ids.shape[-1]\n",
        "\n",
        "        # Create an initial beam of size 1\n",
        "        beams = [(input_ids, tf.constant(0.0, dtype=tf.float32))]\n",
        "\n",
        "        for k in range(iterations):\n",
        "            all_candidates = []\n",
        "            # Expand each beam\n",
        "            for beam_input_ids, beam_score in beams:\n",
        "                logits = self.model(beam_input_ids)\n",
        "                last_token_logits = logits[:, -1, :]\n",
        "                # Use top-k sampling to get the most likely next tokens\n",
        "                top_k = tf.math.top_k(last_token_logits, self.beam_width)\n",
        "                next_token_ids = top_k.indices\n",
        "                log_probs = top_k.values\n",
        "\n",
        "                for i in range(self.beam_width):\n",
        "                    if self.end_token is not None and next_token_ids[0, i] == self.end_token:\n",
        "                        # End of sequence\n",
        "                        all_candidates.append((beam_input_ids, beam_score))\n",
        "                        continue\n",
        "\n",
        "                    new_beam_input_ids = tf.concat([beam_input_ids, tf.reshape(next_token_ids[0, i], (1, 1))], axis=-1)\n",
        "                    new_beam_score = beam_score - log_probs[0, i]\n",
        "                    all_candidates.append((new_beam_input_ids, new_beam_score))\n",
        "\n",
        "            # Select the top-k candidates from all expanded beams\n",
        "            all_candidates = sorted(all_candidates, key=lambda x: x[1])\n",
        "            beams = all_candidates[:self.beam_width]\n",
        "\n",
        "        # Return the best beam at the end\n",
        "        best_beam = min(beams, key=lambda x: x[1])\n",
        "        #print(best_beam[0])\n",
        "        return self.tokenizer.detokenize(best_beam[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(input_prompt, sampler_type, beam_width):\n",
        "    \"\"\"\n",
        "    Generate text using a trained GPT model.\n",
        "\n",
        "    Args:\n",
        "        input_prompt (str): The input text prompt for text generation.\n",
        "        sampler_type (str): Sampling strategy ('greedy' or 'beam'). Default is beam.\n",
        "        beam_width (int): Beam width for beam search sampling. Default is 5.\n",
        "    \"\"\"\n",
        "    model = load_model_and_optimizer(config.model_weights_checkpoint_directory)\n",
        "\n",
        "    if sampler_type == 'beam':\n",
        "        sampler = BeamSearchSampler(model, config.tokenizer_path, config.sequence_length, beam_width=beam_width, end_token=config.end_token)\n",
        "    else:\n",
        "        sampler = GreedySampler(model, config.tokenizer_path, config.sequence_length, end_token=config.end_token)\n",
        "\n",
        "    generated_text = sampler.decode(input_prompt)\n",
        "\n",
        "    # Print the generated text\n",
        "    print(\"Generated Text:\", generated_text)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "lfkpDa_B0h71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "dummy_model = GPT(config)\n",
        "\n",
        "# Generate text using beam search decoding\n",
        "input_prompt = \"This movie is\"\n",
        "\n",
        "generate_response(input_prompt, sampler_type='beam', beam_width=5)"
      ],
      "metadata": {
        "id": "3bkXYIyr7CXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpxrl4wypl-x"
      },
      "source": [
        "**Greedy Approach vs. Beam Search Algorithm: A Comparative Analysis**\n",
        "\n",
        "The greedy approach and the beam search algorithm are two distinct methods for decoding and generating text in natural language processing tasks. Here, we provide a comparative analysis of these approaches, highlighting their differences and relative advantages:\n",
        "\n",
        "**1. Search Strategy:**\n",
        "\n",
        "   - **Greedy Approach:** The greedy approach makes decisions at each step based solely on the highest probability option. It selects the token that appears to be the most likely continuation of the sequence at each step.\n",
        "\n",
        "   - **Beam Search Algorithm:** Beam search, on the other hand, maintains multiple candidate sequences, or \"beams,\" and explores a set number of top-ranked possibilities at each step. It retains a fixed number of promising sequences throughout decoding.\n",
        "\n",
        "**2. Exploration of Possibilities:**\n",
        "\n",
        "   - **Greedy Approach:** Greedy decoding is deterministic and may quickly converge to a locally optimal solution. It tends to produce more deterministic and less diverse output.\n",
        "\n",
        "   - **Beam Search Algorithm:** Beam search allows for a broader exploration of possibilities. It considers multiple candidate sequences, promoting diversity in generated output.\n",
        "\n",
        "**3. Sequence Length:**\n",
        "\n",
        "   - **Greedy Approach:** Greedy decoding does not consider the global context of the sequence, focusing only on the current step. It may lead to sequences that lack coherence or context.\n",
        "\n",
        "   - **Beam Search Algorithm:** Beam search considers longer-range dependencies by retaining multiple sequences, which can result in more coherent and contextually relevant output.\n",
        "\n",
        "**4. Trade-off:**\n",
        "\n",
        "   - **Greedy Approach:** Greedy decoding is computationally efficient as it only considers one option at each step. However, it may sacrifice output quality for speed.\n",
        "\n",
        "   - **Beam Search Algorithm:** Beam search is more computationally intensive as it maintains multiple beams, potentially slowing down the decoding process. However, it often leads to higher-quality and more diverse output.\n",
        "\n",
        "**5. Output Quality:**\n",
        "\n",
        "   - **Greedy Approach:** The greedy approach can produce decent output but may get stuck in local optima, resulting in repetitive or less creative text.\n",
        "\n",
        "   - **Beam Search Algorithm:** Beam search generally produces higher-quality and more diverse output, making it suitable for tasks where output quality and diversity are crucial.\n",
        "\n",
        "**6. Hyperparameter Dependency:**\n",
        "\n",
        "   - **Greedy Approach:** Greedy decoding typically does not involve many hyperparameters and is relatively straightforward to implement.\n",
        "\n",
        "   - **Beam Search Algorithm:** Beam search requires tuning hyperparameters like beam width, which can impact its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMcopQiZqEMO"
      },
      "source": [
        "### **Sampling Methods: Other ideas for generating text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXUVP3jXmuoL"
      },
      "source": [
        "\n",
        "\n",
        "Top-k and nucleus (or top-p) sampling are two techniques used in text generation to control the diversity and quality of generated text. Both top-k and nucleus sampling strategies help control the randomness and quality of text generation. Top-k is a fixed-size strategy, while nucleus sampling is adaptive based on a probability threshold.They both offer ways to make text generation more controllable and coherent. Here's an explanation of each:\n",
        "\n",
        "1. **Top-k Sampling**:\n",
        "   - **Idea**: In top-k sampling, instead of considering all possible tokens in the vocabulary, you limit the selection to the top-k most likely tokens at each step of generation.\n",
        "   - **How It Works**:\n",
        "     - At each generation step, you calculate the probabilities for all tokens in the vocabulary.\n",
        "     - You then select the top-k tokens with the highest probabilities.\n",
        "     - Randomly sample from this reduced set of k tokens according to their probabilities to choose the next token.\n",
        "   - **Purpose**:\n",
        "     - Top-k sampling helps in producing more focused and deterministic text because it limits the choices to a small set of high-probability tokens.\n",
        "     - It reduces the risk of generating nonsensical or erratic text.\n",
        "\n",
        "2. **Nucleus (Top-p) Sampling**:\n",
        "   - **Idea**: Nucleus sampling, sometimes called top-p sampling, dynamically selects the number of tokens to consider based on a threshold probability p.\n",
        "   - **How It Works**:\n",
        "     - At each generation step, you calculate the probabilities for all tokens in the vocabulary.\n",
        "     - You sort the tokens by probability and keep adding tokens with the highest probabilities until the cumulative probability surpasses the threshold p.\n",
        "     - You then randomly sample from this set of tokens according to their probabilities to choose the next token.\n",
        "   - **Purpose**:\n",
        "     - Nucleus sampling offers a balance between randomness and determinism.\n",
        "     - It allows for dynamic adjustment of the token set based on their probabilities, ensuring that you can generate both focused and diverse text depending on the value of p.\n",
        "     - By setting p closer to 1, you get more diverse text, while setting it closer to 0 makes the generation more deterministic.\n",
        "\n",
        "\n",
        "Both top-k and nucleus sampling strategies help control the randomness and quality of text generation. Top-k is a fixed-size strategy, while nucleus sampling is adaptive based on a probability threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4dsj5aP6wNd"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN4QyLSy6xbr"
      },
      "source": [
        "* [Attention is All You Need - Transformer Model for Machine Translation](https://github.com/AliHaiderAhmad001/Neural-Machine-Translator/blob/main/README.md).\n",
        "* [Natural Language Processing with Transformers](https://www.amazon.com/Natural-Language-Processing-Transformers-Revised/dp/1098136799).\n",
        "* [Improving Language Understanding by Generative Pre-Training](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n",
        "* [Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe).\n",
        "* [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iufwkhmGuYJG",
        "5_vS8BRGMkcv",
        "xGUyRNtCTdNZ",
        "uAG5-LiqtnF3",
        "i4HRfrFyB3Nu",
        "4vlwtDEkB5zm",
        "whplZuQBCFUO",
        "WC-t3cf_CJYH",
        "Ygu8Lp2ICNvy"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
